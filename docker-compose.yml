version: '3.8' # It's good practice to specify a more recent patch version

services:
  legal-gpt:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
        - PIP_RETRIES=10
        - PIP_TIMEOUT=600 # Assuming your Dockerfile is in the current directory
    shm_size: 1gb # Shared memory size for the container
    ports:
      - "8000:8000"   # API port mapping
      - "11434:11434" # Ollama port mapping (optional - expose only if you want to access Ollama directly)
    volumes:
      - ./vector_db:/app/vector_db   # Persist vector database
      - ollama_models:/root/.ollama  # Persist Ollama models
    environment:
      - VECTOR_DB_PATH=/app/vector_db
      - OLLAMA_HOST=http://localhost:11434
      - CONCURRENT_REQUESTS=4
      - REQUEST_DELAY=1.0
      - OLLAMA_TIMEOUT=300
      - HTTP_TIMEOUT=30
      # Resource allocation
      - OLLAMA_NUM_THREADS=2
      - OLLAMA_NUM_GPU=0 # Set to number of GPUs you want to use
      - DEFAULT_MODEL=tinyl # Corrected from 'tinylama' to 'tinyl' based on common model naming
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    restart: unless-stopped

volumes:
  ollama_models: # Named volume for Ollama models
