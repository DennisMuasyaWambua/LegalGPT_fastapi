version: '3'

services:
  legal-gpt:
    build: .
        shm_size: 1gb
ports:
      - "8000:8000"  # API
      - "11434:11434" # Ollama (optional - expose only if you want to access Ollama directly)
    volumes:
      - ./vector_db:/app/vector_db  # Persist vector database
      - ollama_models:/root/.ollama  # Persist Ollama models
    environment:
      - VECTOR_DB_PATH=/app/vector_db
      - OLLAMA_HOST=http://localhost:11434
      - CONCURRENT_REQUESTS=4
      - REQUEST_DELAY=1.0
      - OLLAMA_TIMEOUT=300
      - HTTP_TIMEOUT=30
      # Resource allocation
      - OLLAMA_NUM_THREADS=2
      - OLLAMA_NUM_GPU=0  # Set to number of GPUs you want to use
      - DEFAULT_MODEL=tinyl
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2Glama
    restart: unless-stopped

volumes:
  ollama_models:  # Named volume for Ollama models